{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (1, 384)\n",
      "Embedding array type: <class 'numpy.ndarray'>\n",
      "Embedding shape: (1, 384)\n",
      "Embedding dimensionality: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Temp\\ipykernel_25576\\4098340143.py:66: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  db = FAISS(index=index, embedding_function=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"), docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.documents import Document\n",
    "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# Expanded Personal Information Document (for Task 1.1)\n",
    "documents = [\"\"\"\n",
    "My name is Vuong Loc Truong, and I am a Vietnamese citizen. I was born on October 20, 2001, and I am 23 years old. I am currently pursuing a Master's degree in Data Science and AI, which is my highest level of education thus far. My academic journey has focused on the intricacies of data and artificial intelligence, a field I find both challenging and profoundly rewarding. While I currently have no formal work experience, my involvement in web development has provided me with practical insights into the technological landscape. Presently, I serve as a teaching assistant at Van Lang University, where I contribute to the educational development of students.\n",
    "             \n",
    "My core belief regarding the role of technology in shaping society is that it holds immense potential to enhance the quality of life. By providing solutions to complex problems in healthcare, education, and environmental sustainability, technology can be a powerful force for good. However, I also recognize the importance of ensuring equitable access and addressing ethical concerns to prevent the exacerbation of existing inequalities. Responsible innovation and thoughtful regulation are crucial for harnessing technology’s power for the greater good.\n",
    "\n",
    "Furthermore, I believe that cultural values should significantly influence technological advancements. Technology should be developed and implemented in a manner that respects and preserves diverse cultural identities and traditions. Avoiding the imposition of a single cultural perspective is essential. Instead, prioritizing inclusivity and adaptability to different cultural contexts ensures that technology serves the needs of diverse communities and promotes cultural understanding.\n",
    "\n",
    "As a Master's student, I find that the most challenging aspect of my studies thus far is English communication. Overcoming this obstacle is a priority for me. My primary academic goal is to graduate on time, ensuring that I can effectively apply my knowledge and contribute to the field of Data Science and AI. I am dedicated to my studies and eager to see how my research interests will evolve and contribute to the technological advancements of the future.\n",
    "\n",
    "In addition to my academic pursuits, I have a strong interest in web development. This interest has led me to work on various projects, both personal and academic, that involve creating and maintaining websites. My experience in web development has provided me with a solid understanding of front-end and back-end technologies, as well as the importance of user experience and accessibility. I believe that web development is a crucial skill in today's digital age, and I am committed to continuing my growth in this area.\n",
    "\n",
    "At Van Lang University, I have had the opportunity to work closely with students as a teaching assistant. This role has allowed me to develop my communication and mentoring skills, as well as gain a deeper understanding of the educational process. I take great pride in helping students achieve their academic goals and am always looking for ways to improve my teaching methods.\n",
    "\n",
    "Looking ahead, I am excited about the potential for technology to drive positive change in society. I am particularly interested in exploring how data science and artificial intelligence can be used to address pressing global challenges, such as climate change, healthcare, and education. I am committed to using my skills and knowledge to contribute to these efforts and to make a meaningful impact on the world.\n",
    "\"\"\"]\n",
    "\n",
    "# Task 1.1 - Find all relevant sources related to yourself\n",
    "# The personal information document (above) is the relevant source that describes the user's information.\n",
    "\n",
    "# Alternative: Using SentenceTransformers to generate embeddings locally (no API limit)\n",
    "def create_embeddings_with_sentence_transformer(documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Pre-trained model\n",
    "    embeddings = model.encode(documents)\n",
    "    print(f\"Generated embeddings shape: {np.array(embeddings).shape}\")  # Debugging line to check dimensions\n",
    "    return embeddings\n",
    "\n",
    "# Create embeddings using SentenceTransformer embeddings (offline)\n",
    "embeddings = create_embeddings_with_sentence_transformer(documents)\n",
    "\n",
    "# Convert the embeddings to a numpy array (required by FAISS)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "print(f\"Embedding array type: {type(embeddings)}\")  # Debugging line to check the type\n",
    "\n",
    "# Debugging: Print the shape of the embeddings\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Create the FAISS index\n",
    "dimension = embeddings.shape[1]  # The dimensionality of the embeddings\n",
    "print(f\"Embedding dimensionality: {dimension}\")  # Debugging line\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)  # Use L2 distance for similarity search\n",
    "\n",
    "# Add the embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index if necessary\n",
    "faiss.write_index(index, \"faiss_index.index\")\n",
    "\n",
    "# Create the docstore and the index_to_docstore_id dictionary\n",
    "index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "docstore = InMemoryDocstore({index_to_docstore_id[i]: Document(page_content=doc) for i, doc in enumerate(documents)})\n",
    "\n",
    "# Create the FAISS retriever manually using the index and other required arguments\n",
    "db = FAISS(index=index, embedding_function=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"), docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n",
    "\n",
    "# Use FAISS to store embeddings and enable retrieval\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Load the model and tokenizer for QA\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "# Define QA pipeline\n",
    "qa_pipeline = pipeline('question-answering', model=qa_model, tokenizer=qa_tokenizer)\n",
    "\n",
    "# Task 1.2 - Design the prompt template for the chatbot\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are an AI assistant providing precise and concise answers about Vuong Loc Truong.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Task 1.3 - Explore other text-generation models or OPENAI models to enhance capabilities (GPT-2 is being used here as an example)\n",
    "local_model = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def analyze_model_output_v2(question):\n",
    "    \"\"\"Test retrieval and generation with a QA model (DistilBERT).\"\"\"\n",
    "    \n",
    "    # Get relevant documents from the retriever\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Debugging: Print the retrieved documents\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for doc in retrieved_docs:\n",
    "        print(f\"Document: {doc.page_content[:300]}...\")  # Print the first 300 characters for brevity\n",
    "\n",
    "    # Combine the context from the relevant documents\n",
    "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Ask the model to answer the question using the context\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    \n",
    "    # Print the answer\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    return retrieved_docs, result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Temp\\ipykernel_25576\\4098340143.py:95: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Document: \n",
      "My name is Vuong Loc Truong, and I am a Vietnamese citizen. Born on October 20, 2001, now I'm 23, I am currently pursuing a Master's degree in Data Science and AI, marking my highest level of education thus far. My academic journey has been focused on the intricacies of data and artificial intellig...\n",
      "\n",
      "Generated Answer:\n",
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([Document(metadata={}, page_content=\"\\nMy name is Vuong Loc Truong, and I am a Vietnamese citizen. Born on October 20, 2001, now I'm 23, I am currently pursuing a Master's degree in Data Science and AI, marking my highest level of education thus far. My academic journey has been focused on the intricacies of data and artificial intelligence, a field I find both challenging and profoundly rewarding. While I currently have no formal years of work experience, my involvement in web development has provided me with practical insights into the technological landscape. Presently, I serve as a teaching assistant at Van Lang University, where I contribute to the educational development of students.\\n\\nMy core belief regarding the role of technology in shaping society is that it holds immense potential to enhance the quality of life. By providing solutions to complex problems in healthcare, education, and environmental sustainability, technology can be a powerful force for good. However, I also recognize the importance of ensuring equitable access and addressing ethical concerns to prevent the exacerbation of existing inequalities. Responsible innovation and thoughtful regulation are crucial for harnessing technology’s power for the greater good.\\n\\nFurthermore, I believe that cultural values should significantly influence technological advancements. Technology should be developed and implemented in a manner that respects and preserves diverse cultural identities and traditions. Avoiding the imposition of a single cultural perspective is essential. Instead, prioritizing inclusivity and adaptability to different cultural contexts ensures that technology serves the needs of diverse communities and promotes cultural understanding.\\n\\nAs a Master's student, I find that the most challenging aspect of my studies thus far is English communication. Overcoming this obstacle is a priority for me. My primary academic goal is to graduate on time, ensuring that I can effectively apply my knowledge and contribute to the field of Data Science and AI. I am dedicated to my studies and eager to see how my research interests will evolve and contribute to the technological advancements of the future.\\n\\nIn addition to my academic pursuits, I have a strong interest in web development. This interest has led me to work on various projects, both personal and academic, that involve creating and maintaining websites. My experience in web development has provided me with a solid understanding of front-end and back-end technologies, as well as the importance of user experience and accessibility. I believe that web development is a crucial skill in today's digital age, and I am committed to continuing my growth in this area.\\n\\nAt Van Lang University, I have had the opportunity to work closely with students as a teaching assistant. This role has allowed me to develop my communication and mentoring skills, as well as gain a deeper understanding of the educational process. I take great pride in helping students achieve their academic goals and am always looking for ways to improve my teaching methods.\\n\\nLooking ahead, I am excited about the potential for technology to drive positive change in society. I am particularly interested in exploring how data science and artificial intelligence can be used to address pressing global challenges, such as climate change, healthcare, and education. I am committed to using my skills and knowledge to contribute to these efforts and to make a meaningful impact on the world.\\n\")],\n",
       " '23')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1.3 - Example Question\n",
    "analyze_model_output_v2(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Models Used: ['FAISS with SentenceTransformerEmbeddings']\n",
      "Generator Models Used: ['DistilBERT QA model', 'HuggingFace GPT-2']\n",
      "Issues Found:\n",
      "Issue with FAISS with SentenceTransformerEmbeddings: Potential for retrieving unrelated documents due to embedding distance inaccuracies.\n",
      "Issue with HuggingFace GPT-2: Potential for generating unrelated information due to model's generalization capabilities.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Task 2.1 - List of retriever and generator models used\n",
    "retriever_models = [\"FAISS with SentenceTransformerEmbeddings\"]\n",
    "generator_models = [\"DistilBERT QA model\", \"HuggingFace GPT-2\"]\n",
    "\n",
    "print(\"Retriever Models Used:\", retriever_models)\n",
    "print(\"Generator Models Used:\", generator_models)\n",
    "\n",
    "# Task 2.2 - Analyze any issues related to the models providing unrelated information\n",
    "def analyze_issues(retriever_models, generator_models):\n",
    "    issues = []\n",
    "    \n",
    "    # Check for issues in retriever models\n",
    "    for model in retriever_models:\n",
    "        if \"FAISS\" in model:\n",
    "            issues.append(f\"Issue with {model}: Potential for retrieving unrelated documents due to embedding distance inaccuracies.\")\n",
    "    \n",
    "    # Check for issues in generator models\n",
    "    for model in generator_models:\n",
    "        if \"GPT-2\" in model:\n",
    "            issues.append(f\"Issue with {model}: Potential for generating unrelated information due to model's generalization capabilities.\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Task 2.2 - Example issue analysis\n",
    "issues = analyze_issues(retriever_models, generator_models)\n",
    "print(\"Issues Found:\")\n",
    "for issue in issues:\n",
    "    print(issue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
